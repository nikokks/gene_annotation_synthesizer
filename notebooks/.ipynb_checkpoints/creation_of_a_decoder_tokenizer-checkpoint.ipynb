{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ab79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6109941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/txt_files/’: File exists\n",
      "mkdir: cannot create directory ‘../data/tokenizer_folder/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir '../data/txt_files/'\n",
    "!mkdir '../data/tokenizer_folder/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d31c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files_dir = '../data/txt_files/'\n",
    "tokenizer_folder = '../data/tokenizer_folder/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4823158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/names.tsv.gz',compression='gzip',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4c37d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "160a2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 'float' object has no attribute 'encode'\n",
      "nan 'float' object has no attribute 'encode'\n"
     ]
    }
   ],
   "source": [
    "# Store values in a dataframe column (Series object) to files, one file per record\n",
    "def column_to_files(column, prefix, txt_files_dir):\n",
    "    # The prefix is a unique ID to avoid to overwrite a text file\n",
    "    i=prefix\n",
    "    #For every value in the df, with just one column\n",
    "    for row in column.to_list():\n",
    "        # Create the filename using the prefix ID\n",
    "        file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
    "        try:\n",
    "            # Create the file and write the column text to it\n",
    "            f = open(file_name, 'wb')\n",
    "            f.write(row.encode('utf-8'))\n",
    "            f.close()\n",
    "        except Exception as e:  #catch exceptions(for eg. empty rows)\n",
    "            print(row, e) \n",
    "        i+=1\n",
    "    # Return the last ID\n",
    "    return i\n",
    "# Get the training data\n",
    "data = train_df[\"name\"]\n",
    "# Removing the end of line character \\n\n",
    "data = data.replace(\"\\n\",\" \")\n",
    "# Set the ID to 0\n",
    "prefix=0\n",
    "\n",
    "# Create a file for every description value\n",
    "prefix = column_to_files(data, prefix, txt_files_dir)\n",
    "\n",
    "# Removing the end of line character \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7580198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(txt_files_dir+\"*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=10000, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10d9c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tokenizer_folder/vocab.json', '../data/tokenizer_folder/merges.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the Tokenizer to disk\n",
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f6d529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'te', 'st', '</s>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tokenizer using vocab.json and mrege.txt files\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
    ")\n",
    "# Prepare the tokenizer\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# Test the tokenizer\n",
    "# Show the tokens created\n",
    "tokenizer.encode(\"test\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b75dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
